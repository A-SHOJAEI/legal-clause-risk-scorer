{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Clause Risk Scorer - Data Exploration and Model Development\n",
    "\n",
    "This notebook provides comprehensive exploration of legal contract data and demonstrates the capabilities of our risk assessment model.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading and Exploration](#data-exploration)\n",
    "3. [Text Preprocessing Analysis](#preprocessing)\n",
    "4. [Model Architecture Overview](#model-architecture)\n",
    "5. [Training Demonstration](#training)\n",
    "6. [Risk Assessment Examples](#risk-examples)\n",
    "7. [Performance Analysis](#performance)\n",
    "8. [Interactive Risk Scoring](#interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Legal risk scorer imports\n",
    "from legal_clause_risk_scorer.utils.config import load_config, set_random_seeds\n",
    "from legal_clause_risk_scorer.data.loader import LegalDataLoader\n",
    "from legal_clause_risk_scorer.data.preprocessing import LegalTextPreprocessor\n",
    "from legal_clause_risk_scorer.models.model import LegalClauseRiskModel\n",
    "from legal_clause_risk_scorer.evaluation.metrics import LegalRiskMetrics\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "set_random_seeds(config)\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"  Base model: {config.get('model.base_model')}\")\n",
    "print(f\"  Batch size: {config.get('training.batch_size')}\")\n",
    "print(f\"  Learning rate: {config.get('training.learning_rate')}\")\n",
    "print(f\"  Target F1: {config.get('evaluation.targets.clause_detection_f1')}\")\n",
    "print(f\"  Target MAE: {config.get('evaluation.targets.risk_score_mae')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration {#data-exploration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = LegalDataLoader(config)\n",
    "print(\"üìÅ Data loader initialized\")\n",
    "\n",
    "# Load sample data for exploration (smaller subset for notebook)\n",
    "try:\n",
    "    # Load a small sample of CUAD data first\n",
    "    print(\"Loading sample legal contract data...\")\n",
    "    cuad_sample = data_loader.load_cuad_dataset()\n",
    "    print(f\"‚úÖ Loaded {len(cuad_sample)} CUAD samples\")\n",
    "    \n",
    "    # Convert to pandas for exploration\n",
    "    cuad_df = cuad_sample.to_pandas()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load CUAD data: {e}\")\n",
    "    print(\"Creating synthetic sample data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic legal clause data for demonstration\n",
    "    synthetic_clauses = [\n",
    "        {\n",
    "            'text': \"The Employee may terminate this agreement at any time with 30 days written notice.\",\n",
    "            'clause_type': 'termination',\n",
    "            'risk_score': 3.2,\n",
    "            'risk_category': 'low_risk'\n",
    "        },\n",
    "        {\n",
    "            'text': \"The Company may terminate Employee's employment at any time, with or without cause, and with or without notice.\",\n",
    "            'clause_type': 'termination',\n",
    "            'risk_score': 8.7,\n",
    "            'risk_category': 'high_risk'\n",
    "        },\n",
    "        {\n",
    "            'text': \"Employee agrees not to compete with the Company for a period of 2 years following termination.\",\n",
    "            'clause_type': 'non_compete',\n",
    "            'risk_score': 7.5,\n",
    "            'risk_category': 'high_risk'\n",
    "        },\n",
    "        {\n",
    "            'text': \"All intellectual property created during employment shall belong to the Company.\",\n",
    "            'clause_type': 'intellectual_property',\n",
    "            'risk_score': 6.2,\n",
    "            'risk_category': 'medium_risk'\n",
    "        },\n",
    "        {\n",
    "            'text': \"Employee shall receive base salary plus performance bonuses as determined by management.\",\n",
    "            'clause_type': 'compensation',\n",
    "            'risk_score': 4.8,\n",
    "            'risk_category': 'medium_risk'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    cuad_df = pd.DataFrame(synthetic_clauses)\n",
    "    print(f\"‚úÖ Created {len(cuad_df)} synthetic samples\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(cuad_df.info())\n",
    "cuad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze risk score distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Risk score histogram\n",
    "axes[0].hist(cuad_df['risk_score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Risk Score Distribution')\n",
    "axes[0].set_xlabel('Risk Score (1-10)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Risk category distribution\n",
    "risk_counts = cuad_df['risk_category'].value_counts()\n",
    "axes[1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Risk Category Distribution')\n",
    "\n",
    "# Clause type distribution\n",
    "clause_counts = cuad_df['clause_type'].value_counts()\n",
    "axes[2].bar(clause_counts.index, clause_counts.values, color='lightcoral')\n",
    "axes[2].set_title('Clause Type Distribution')\n",
    "axes[2].set_xlabel('Clause Type')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Risk Score Statistics:\")\n",
    "print(cuad_df['risk_score'].describe())\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Risk Category Counts:\")\n",
    "print(cuad_df['risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics\n",
    "cuad_df['text_length'] = cuad_df['text'].str.len()\n",
    "cuad_df['word_count'] = cuad_df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0].hist(cuad_df['text_length'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(cuad_df['word_count'], bins=20, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìè Text Statistics:\")\n",
    "print(f\"Average text length: {cuad_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {cuad_df['word_count'].mean():.1f} words\")\n",
    "print(f\"Longest text: {cuad_df['text_length'].max()} characters\")\n",
    "print(f\"Shortest text: {cuad_df['text_length'].min()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Analysis {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text preprocessor\n",
    "preprocessor = LegalTextPreprocessor(config)\n",
    "print(\"üîß Text preprocessor initialized\")\n",
    "\n",
    "# Demonstrate preprocessing on sample texts\n",
    "sample_texts = [\n",
    "    \"The Employee shall not, during the term of employment and for two (2) years thereafter, directly or indirectly compete with the Company.\",\n",
    "    \"This Agreement shall terminate upon 30 days written notice by either party, provided that such termination shall not affect any accrued obligations.\",\n",
    "    \"All intellectual property, including but not limited to patents, copyrights, and trade secrets, developed during employment shall be the exclusive property of Company.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüßπ Preprocessing Examples:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    cleaned = preprocessor.clean_legal_text(text)\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(f\"Length change: {len(text)} ‚Üí {len(cleaned)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for analysis\n",
    "if len(cuad_df) > 0:\n",
    "    sample_texts_for_features = cuad_df['text'].tolist()[:10]  # Use first 10 samples\n",
    "    \n",
    "    print(\"üîç Extracting text features...\")\n",
    "    features = preprocessor.extract_features(sample_texts_for_features)\n",
    "    \n",
    "    # Create features DataFrame\n",
    "    features_df = pd.DataFrame({\n",
    "        'text_length': features['text_length'],\n",
    "        'word_count': features['word_count'],\n",
    "        'sentence_count': features['sentence_count'],\n",
    "        'legal_terms_count': features['legal_terms_count'],\n",
    "        'risk_keywords_count': features['risk_keywords_count'],\n",
    "        'modal_verbs_count': features['modal_verbs_count'],\n",
    "        'avg_sentence_length': features['avg_sentence_length'],\n",
    "        'avg_word_length': features['avg_word_length']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìä Feature Statistics:\")\n",
    "    print(features_df.describe())\n",
    "    \n",
    "    # Visualize feature correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = features_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Overview {#model-architecture}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for architecture exploration\n",
    "print(\"üèóÔ∏è Initializing model architecture...\")\n",
    "\n",
    "try:\n",
    "    model = LegalClauseRiskModel(config)\n",
    "    print(\"‚úÖ Model initialized successfully\")\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(\"\\nüîç Model Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Count parameters\n",
    "    from legal_clause_risk_scorer.models.model import count_parameters\n",
    "    param_counts = count_parameters(model)\n",
    "    \n",
    "    print(\"\\nüìä Model Parameters:\")\n",
    "    print(f\"  Total parameters: {param_counts['total_parameters']:,}\")\n",
    "    print(f\"  Trainable parameters: {param_counts['trainable_parameters']:,}\")\n",
    "    print(f\"  Frozen parameters: {param_counts['frozen_parameters']:,}\")\n",
    "    \n",
    "    # Model configuration details\n",
    "    print(\"\\n‚öôÔ∏è Model Configuration:\")\n",
    "    print(f\"  Base model: {config.get('model.base_model')}\")\n",
    "    print(f\"  Hidden size: {config.get('model.hidden_size')}\")\n",
    "    print(f\"  Number of labels: {config.get('model.num_labels')}\")\n",
    "    print(f\"  Dropout rate: {config.get('model.dropout')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not initialize full model: {e}\")\n",
    "    print(\"This may be due to missing model weights or dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Demonstration {#training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate training pipeline setup\n",
    "print(\"üöÇ Training Pipeline Demonstration\")\n",
    "print(\"(Note: This is a setup demonstration, not actual training)\")\n",
    "\n",
    "# Show training configuration\n",
    "training_config = config.get_section('training')\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Demonstrate data loading for training\n",
    "print(\"\\nüìä Data Loading for Training:\")\n",
    "print(\"1. Load CUAD and LEDGAR datasets\")\n",
    "print(\"2. Combine and balance datasets\")\n",
    "print(\"3. Create train/validation/test splits\")\n",
    "print(\"4. Tokenize text data\")\n",
    "print(\"5. Create PyTorch DataLoaders\")\n",
    "\n",
    "# Show evaluation configuration\n",
    "evaluation_config = config.get_section('evaluation')\n",
    "print(\"\\nüéØ Evaluation Configuration:\")\n",
    "print(f\"  Metrics: {evaluation_config['metrics']}\")\n",
    "print(f\"  Targets: {evaluation_config['targets']}\")\n",
    "\n",
    "# MLflow configuration\n",
    "mlflow_config = config.get_section('mlflow')\n",
    "print(\"\\nüìà Experiment Tracking:\")\n",
    "for key, value in mlflow_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk Assessment Examples {#risk-examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example legal clauses with varying risk levels\n",
    "example_clauses = [\n",
    "    {\n",
    "        'text': \"Employee may terminate this agreement with 30 days written notice.\",\n",
    "        'expected_risk': 'low',\n",
    "        'explanation': \"Gives employee reasonable termination rights\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Company may terminate employee immediately without cause or notice.\",\n",
    "        'expected_risk': 'high',\n",
    "        'explanation': \"At-will termination with no protection for employee\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Employee agrees to non-disclosure of proprietary information during and after employment.\",\n",
    "        'expected_risk': 'medium',\n",
    "        'explanation': \"Standard confidentiality clause\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Employee shall not compete with company for 5 years in any capacity worldwide.\",\n",
    "        'expected_risk': 'high',\n",
    "        'explanation': \"Overly broad non-compete restriction\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Salary will be reviewed annually with potential for merit-based increases.\",\n",
    "        'expected_risk': 'low',\n",
    "        'explanation': \"Fair compensation review process\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize risk assessment components\n",
    "print(\"‚öñÔ∏è Legal Clause Risk Assessment Examples\")\n",
    "print(\"\\nAnalyzing example clauses for risk indicators...\")\n",
    "\n",
    "# Manual risk scoring based on keywords and patterns\n",
    "def assess_clause_risk(text):\n",
    "    \"\"\"Simple rule-based risk assessment for demonstration.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    high_risk_indicators = [\n",
    "        'without cause', 'without notice', 'immediately', 'at will',\n",
    "        'sole discretion', 'worldwide', 'unlimited', 'perpetual',\n",
    "        'waive', 'forfeit', 'irrevocable'\n",
    "    ]\n",
    "    \n",
    "    protective_indicators = [\n",
    "        'with notice', 'reasonable', 'fair', 'written notice',\n",
    "        'mutual', 'subject to', 'limited', 'annual review'\n",
    "    ]\n",
    "    \n",
    "    risk_score = 5.0  # Neutral starting point\n",
    "    \n",
    "    for indicator in high_risk_indicators:\n",
    "        if indicator in text_lower:\n",
    "            risk_score += 1.5\n",
    "    \n",
    "    for indicator in protective_indicators:\n",
    "        if indicator in text_lower:\n",
    "            risk_score -= 1.0\n",
    "    \n",
    "    # Clamp to 1-10 range\n",
    "    return max(1.0, min(10.0, risk_score))\n",
    "\n",
    "# Assess each example clause\n",
    "results_df = []\n",
    "for i, clause in enumerate(example_clauses, 1):\n",
    "    risk_score = assess_clause_risk(clause['text'])\n",
    "    \n",
    "    if risk_score <= 3.5:\n",
    "        risk_category = 'low_risk'\n",
    "        color = 'üü¢'\n",
    "    elif risk_score <= 7.0:\n",
    "        risk_category = 'medium_risk'\n",
    "        color = 'üü°'\n",
    "    else:\n",
    "        risk_category = 'high_risk'\n",
    "        color = 'üî¥'\n",
    "    \n",
    "    print(f\"\\n--- Example {i} {color} ---\")\n",
    "    print(f\"Text: {clause['text']}\")\n",
    "    print(f\"Risk Score: {risk_score:.1f}/10\")\n",
    "    print(f\"Risk Category: {risk_category}\")\n",
    "    print(f\"Expected: {clause['expected_risk']} risk\")\n",
    "    print(f\"Explanation: {clause['explanation']}\")\n",
    "    \n",
    "    results_df.append({\n",
    "        'clause': clause['text'][:50] + '...' if len(clause['text']) > 50 else clause['text'],\n",
    "        'risk_score': risk_score,\n",
    "        'risk_category': risk_category,\n",
    "        'expected': clause['expected_risk']\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(results_df)\n",
    "print(\"\\nüìä Risk Assessment Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk assessment results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Risk score comparison\n",
    "clause_nums = range(1, len(results_df) + 1)\n",
    "colors = ['green' if cat == 'low_risk' else 'orange' if cat == 'medium_risk' else 'red' \n",
    "          for cat in results_df['risk_category']]\n",
    "\n",
    "axes[0].bar(clause_nums, results_df['risk_score'], color=colors, alpha=0.7)\n",
    "axes[0].set_title('Risk Scores by Clause')\n",
    "axes[0].set_xlabel('Clause Number')\n",
    "axes[0].set_ylabel('Risk Score (1-10)')\n",
    "axes[0].set_ylim(0, 10)\n",
    "\n",
    "# Add horizontal lines for risk thresholds\n",
    "axes[0].axhline(y=3.5, color='orange', linestyle='--', alpha=0.5, label='Low/Medium threshold')\n",
    "axes[0].axhline(y=7.0, color='red', linestyle='--', alpha=0.5, label='Medium/High threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Risk category distribution\n",
    "category_counts = results_df['risk_category'].value_counts()\n",
    "axes[1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "           colors=['green', 'orange', 'red'])\n",
    "axes[1].set_title('Risk Category Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Assessment Accuracy:\")\n",
    "expected_map = {'low': 'low_risk', 'medium': 'medium_risk', 'high': 'high_risk'}\n",
    "results_df['expected_category'] = results_df['expected'].map(expected_map)\n",
    "accuracy = (results_df['risk_category'] == results_df['expected_category']).mean()\n",
    "print(f\"Category prediction accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics evaluator\n",
    "metrics_evaluator = LegalRiskMetrics(config)\n",
    "print(\"üìä Performance Analysis\")\n",
    "\n",
    "# Simulate evaluation results for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate synthetic evaluation data\n",
    "y_true_class = np.random.choice([0, 1, 2], size=n_samples, p=[0.4, 0.35, 0.25])\n",
    "y_pred_class = y_true_class + np.random.choice([-1, 0, 1], size=n_samples, p=[0.1, 0.8, 0.1])\n",
    "y_pred_class = np.clip(y_pred_class, 0, 2)\n",
    "\n",
    "y_true_score = np.random.normal(5.0, 2.0, n_samples)\n",
    "y_true_score = np.clip(y_true_score, 1, 10)\n",
    "y_pred_score = y_true_score + np.random.normal(0, 0.8, n_samples)\n",
    "y_pred_score = np.clip(y_pred_score, 1, 10)\n",
    "\n",
    "# Generate prediction probabilities\n",
    "y_prob = np.random.dirichlet([2, 2, 2], size=n_samples)\n",
    "# Adjust probabilities to be more consistent with predictions\n",
    "for i in range(n_samples):\n",
    "    y_prob[i, y_pred_class[i]] = max(y_prob[i, y_pred_class[i]], 0.6)\n",
    "    y_prob[i] = y_prob[i] / y_prob[i].sum()  # Renormalize\n",
    "\n",
    "print(f\"\\nüî¢ Simulated evaluation data: {n_samples} samples\")\n",
    "print(f\"True class distribution: {np.bincount(y_true_class)}\")\n",
    "print(f\"Predicted class distribution: {np.bincount(y_pred_class)}\")\n",
    "print(f\"True score range: {y_true_score.min():.1f} - {y_true_score.max():.1f}\")\n",
    "print(f\"Predicted score range: {y_pred_score.min():.1f} - {y_pred_score.max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multi-task performance\n",
    "evaluation_results = metrics_evaluator.evaluate_multi_task(\n",
    "    classification_true=y_true_class,\n",
    "    classification_pred=y_pred_class,\n",
    "    regression_true=y_true_score,\n",
    "    regression_pred=y_pred_score,\n",
    "    classification_prob=y_prob\n",
    ")\n",
    "\n",
    "print(\"\\nüìà Classification Performance:\")\n",
    "class_metrics = evaluation_results['classification']\n",
    "print(f\"  Accuracy: {class_metrics['accuracy']:.3f}\")\n",
    "print(f\"  Precision (macro): {class_metrics['precision_macro']:.3f}\")\n",
    "print(f\"  Recall (macro): {class_metrics['recall_macro']:.3f}\")\n",
    "print(f\"  F1 Score (macro): {class_metrics['f1_macro']:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Regression Performance:\")\n",
    "regr_metrics = evaluation_results['regression']\n",
    "print(f\"  MAE: {regr_metrics['mae']:.3f}\")\n",
    "print(f\"  RMSE: {regr_metrics['rmse']:.3f}\")\n",
    "print(f\"  R¬≤ Score: {regr_metrics['r2']:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Target Compliance:\")\n",
    "target_f1 = config.get('evaluation.targets.clause_detection_f1', 0.82)\n",
    "target_mae = config.get('evaluation.targets.risk_score_mae', 1.2)\n",
    "\n",
    "f1_status = \"‚úÖ PASS\" if class_metrics['f1_macro'] >= target_f1 else \"‚ùå FAIL\"\n",
    "mae_status = \"‚úÖ PASS\" if regr_metrics['mae'] <= target_mae else \"‚ùå FAIL\"\n",
    "\n",
    "print(f\"  F1 Score: {class_metrics['f1_macro']:.3f} vs target {target_f1:.3f} {f1_status}\")\n",
    "print(f\"  MAE: {regr_metrics['mae']:.3f} vs target {target_mae:.3f} {mae_status}\")\n",
    "\n",
    "print(f\"\\nüèÜ Overall Performance Score: {evaluation_results['overall_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Classification confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "class_names = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0,0])\n",
    "axes[0,0].set_title('Classification Confusion Matrix')\n",
    "axes[0,0].set_ylabel('True Label')\n",
    "axes[0,0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Regression scatter plot\n",
    "axes[0,1].scatter(y_true_score, y_pred_score, alpha=0.6, c='blue')\n",
    "axes[0,1].plot([1, 10], [1, 10], 'r--', alpha=0.8)\n",
    "axes[0,1].set_title('Regression: True vs Predicted Scores')\n",
    "axes[0,1].set_xlabel('True Risk Score')\n",
    "axes[0,1].set_ylabel('Predicted Risk Score')\n",
    "axes[0,1].set_xlim(1, 10)\n",
    "axes[0,1].set_ylim(1, 10)\n",
    "\n",
    "# Regression residuals\n",
    "residuals = y_pred_score - y_true_score\n",
    "axes[1,0].scatter(y_true_score, residuals, alpha=0.6, c='green')\n",
    "axes[1,0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1,0].set_title('Regression Residuals')\n",
    "axes[1,0].set_xlabel('True Risk Score')\n",
    "axes[1,0].set_ylabel('Prediction Error')\n",
    "\n",
    "# Performance metrics comparison\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "metrics_values = [\n",
    "    class_metrics['accuracy'],\n",
    "    class_metrics['precision_macro'],\n",
    "    class_metrics['recall_macro'],\n",
    "    class_metrics['f1_macro']\n",
    "]\n",
    "target_values = [0.85, 0.80, 0.80, target_f1]  # Example targets\n",
    "\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,1].bar(x_pos - width/2, metrics_values, width, label='Actual', color='skyblue')\n",
    "axes[1,1].bar(x_pos + width/2, target_values, width, label='Target', color='lightcoral')\n",
    "axes[1,1].set_title('Classification Metrics vs Targets')\n",
    "axes[1,1].set_xlabel('Metrics')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_xticks(x_pos)\n",
    "axes[1,1].set_xticklabels(metrics_names, rotation=45)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Risk Scoring {#interactive}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive risk assessment function\n",
    "def interactive_risk_assessment(text):\n",
    "    \"\"\"Assess risk for user-provided legal clause.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Please provide a legal clause to analyze.\"\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = preprocessor.clean_legal_text(text)\n",
    "    \n",
    "    # Simple rule-based risk assessment (in production, this would use the trained model)\n",
    "    risk_score = assess_clause_risk(text)\n",
    "    \n",
    "    if risk_score <= 3.5:\n",
    "        risk_category = 'LOW RISK üü¢'\n",
    "        recommendation = \"This clause appears favorable to the employee.\"\n",
    "    elif risk_score <= 7.0:\n",
    "        risk_category = 'MEDIUM RISK üü°'\n",
    "        recommendation = \"This clause has some concerning elements. Review carefully.\"\n",
    "    else:\n",
    "        risk_category = 'HIGH RISK üî¥'\n",
    "        recommendation = \"This clause may be heavily skewed against the employee. Consider negotiation.\"\n",
    "    \n",
    "    # Identify risk indicators\n",
    "    text_lower = text.lower()\n",
    "    high_risk_found = []\n",
    "    protective_found = []\n",
    "    \n",
    "    high_risk_indicators = [\n",
    "        'without cause', 'without notice', 'immediately', 'at will',\n",
    "        'sole discretion', 'worldwide', 'unlimited', 'perpetual'\n",
    "    ]\n",
    "    \n",
    "    protective_indicators = [\n",
    "        'with notice', 'reasonable', 'fair', 'written notice',\n",
    "        'mutual', 'subject to', 'limited'\n",
    "    ]\n",
    "    \n",
    "    for indicator in high_risk_indicators:\n",
    "        if indicator in text_lower:\n",
    "            high_risk_found.append(indicator)\n",
    "    \n",
    "    for indicator in protective_indicators:\n",
    "        if indicator in text_lower:\n",
    "            protective_found.append(indicator)\n",
    "    \n",
    "    # Format results\n",
    "    results = f\"\"\"\n",
    "üìÑ LEGAL CLAUSE RISK ASSESSMENT\n",
    "{'='*50}\n",
    "\n",
    "Original Text:\n",
    "{text}\n",
    "\n",
    "Risk Score: {risk_score:.1f}/10\n",
    "Risk Level: {risk_category}\n",
    "\n",
    "Recommendation:\n",
    "{recommendation}\n",
    "\n",
    "Risk Indicators Found:\n",
    "‚ùå High-risk terms: {', '.join(high_risk_found) if high_risk_found else 'None detected'}\n",
    "‚úÖ Protective terms: {', '.join(protective_found) if protective_found else 'None detected'}\n",
    "\n",
    "Analysis Details:\n",
    "‚Ä¢ Text length: {len(text)} characters\n",
    "‚Ä¢ Word count: {len(text.split())} words\n",
    "‚Ä¢ Cleaned text: {cleaned_text[:100]}{'...' if len(cleaned_text) > 100 else ''}\n",
    "    \"\"\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üéÆ Interactive Legal Clause Risk Assessment\")\n",
    "print(\"You can now analyze any legal clause for risk factors!\")\n",
    "print(\"\\nTry analyzing these example clauses:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: High-risk clause\n",
    "high_risk_clause = \"The Company may terminate the Employee at any time, with or without cause, and with or without notice, at the sole discretion of management.\"\n",
    "\n",
    "print(\"üî¥ EXAMPLE 1 - High Risk Clause:\")\n",
    "print(interactive_risk_assessment(high_risk_clause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Low-risk clause\n",
    "low_risk_clause = \"Either party may terminate this agreement with 30 days written notice, subject to completion of current projects and fair transition arrangements.\"\n",
    "\n",
    "print(\"üü¢ EXAMPLE 2 - Low Risk Clause:\")\n",
    "print(interactive_risk_assessment(low_risk_clause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Medium-risk clause\n",
    "medium_risk_clause = \"Employee agrees not to disclose confidential information and not to compete with similar businesses for 12 months after termination.\"\n",
    "\n",
    "print(\"üü° EXAMPLE 3 - Medium Risk Clause:\")\n",
    "print(interactive_risk_assessment(medium_risk_clause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom clause analysis\n",
    "print(\"‚úèÔ∏è Analyze Your Own Clause:\")\n",
    "print(\"Replace the text below with your own legal clause to analyze:\")\n",
    "\n",
    "# User can modify this text\n",
    "custom_clause = \"Enter your legal clause here for risk assessment.\"\n",
    "\n",
    "if custom_clause != \"Enter your legal clause here for risk assessment.\":\n",
    "    print(\"\\nüîç CUSTOM CLAUSE ANALYSIS:\")\n",
    "    print(interactive_risk_assessment(custom_clause))\n",
    "else:\n",
    "    print(\"\\nüí° Tip: Replace the custom_clause variable above with your own legal text to see the risk assessment!\")\n",
    "    print(\"\\nFor example, try:\")\n",
    "    print('custom_clause = \"Your legal clause text here\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've explored the Legal Clause Risk Scorer system.\n",
    "\n",
    "### What We've Covered:\n",
    "1. ‚úÖ **Data Loading**: CUAD and LEDGAR dataset integration\n",
    "2. ‚úÖ **Text Preprocessing**: Legal-specific cleaning and feature extraction\n",
    "3. ‚úÖ **Model Architecture**: Multi-task transformer for classification and regression\n",
    "4. ‚úÖ **Risk Assessment**: Automated scoring of legal clauses\n",
    "5. ‚úÖ **Performance Analysis**: Comprehensive evaluation metrics\n",
    "6. ‚úÖ **Interactive Demo**: Real-time clause analysis\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-task Learning**: Simultaneous classification and risk scoring\n",
    "- **Legal Domain Expertise**: Specialized preprocessing and evaluation\n",
    "- **Production Ready**: MLflow tracking, checkpointing, comprehensive testing\n",
    "- **Interpretable**: Attention weights and feature analysis\n",
    "\n",
    "### Performance Targets:\n",
    "- üéØ **F1 Score**: ‚â• 0.82 for clause detection\n",
    "- üéØ **MAE**: ‚â§ 1.2 for risk score prediction\n",
    "- üéØ **Recall**: ‚â• 0.88 for unfavorable term detection\n",
    "\n",
    "### Next Steps:\n",
    "1. **Train the Model**: Run `python scripts/train.py` to train on full datasets\n",
    "2. **Evaluate Performance**: Use `python scripts/evaluate.py` for comprehensive testing\n",
    "3. **Deploy for Production**: Integrate with legal document review workflows\n",
    "4. **Continuous Learning**: Update model with new legal clause patterns\n",
    "\n",
    "### Usage Examples:\n",
    "```bash\n",
    "# Train the model\n",
    "python scripts/train.py\n",
    "\n",
    "# Evaluate on test data\n",
    "python scripts/evaluate.py --model models/best_model.pth --report\n",
    "\n",
    "# Analyze custom clauses\n",
    "python scripts/evaluate.py --model models/best_model.pth --data my_clauses.csv\n",
    "```\n",
    "\n",
    "---\n",
    "*The Legal Clause Risk Scorer helps identify potentially unfavorable terms in employment contracts and NDAs, focusing on asymmetric risk detection to protect employee interests.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}